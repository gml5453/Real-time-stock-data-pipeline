snowflake DBT Airflow Kafka perhaps you heard of these tools together these tools make a modern data stack and imagine including all these tools in one project one data pipeline and including that project into your portfolio that is a real gamecher and that's exactly what we're going to do today we are going to build a realtime stock market data pipeline not a simulation though but actual live market data we're going to stream it in real time orchestrate it with Airflow store it in Snowflake transform it with DVT and get the life to the data with visualizations in PowerBI and by the end of this video you will see a modern data stack in action end to end and you'll also have a portfolio project that actually stands out so stick with me every minute you spend here actually worth it and if you like the content don't forget to follow with that being said let's begin let's begin with the architecture of our project like any other data pipeline our data pipeline also starts with a data source which is an API application programming interface in our project and that API will be FinHub which provides a stock data for us this data will be later streamed using Kafka as streaming engine and this stream data will be pulled out and stored in an S3 bucket well I'm not going to use an AWS S3 actually but I'm going to use something some alternative to that which is minio which performs identical to an AWS S3 bucket so I'm going to use a minio instead of AWS S3 here for the data storage and this stored data will be pulled out and uploaded into snowflake and that snowflake uploaded data will be initially in the raw stage which is the bronze table and then converted into silver and then converted into gold all this transformation all the way from raw to clean clean data that is stored in silver and business ready transform data that is stored in gold all this data processing is done inside snowflake using dbt as a data processor and snowflake is nothing but a data warehouse which stores the data and then the process will be orchestrated using Apache airflow after that the business ready data that is in goal table in snowflake we're going to use powerbi to connect to snowflake to retrieve the data from there and create a dashboard in powerbi we'll be using docker to uh execute most of the tools here so that is that is it with the architecture and to begin with we need the tools like docker VS code and we also need a snowflake account basically snowflake provides a free trial for anyone so but for the first time when you create the snowflake account they'll give you $400 worth of free credit that you can use let me show you and you'll land in this homepage and here at the bottom you can see the credits that that are left and it's been 6 days since I created this account and you'll you'll be granted with $400 worth of credits and you also need Docker Desktop and Visual Studio firstly let me in let me show you how to install Docker Desktop i'll post by the way I'll post all these links all these links for the tools in the description of this video and also all the codes that I use will be uploaded to GitHub and uh the GitHub link will be provided also in the description well this is the Docker web page here click on the download desktop Docker desktop and choose which processor your computer is using if you have AMD processor choose AMD if it's Intel ARM so mine is AMD i'm going to download this you know what so it's like 500 MB 500 MGAB i'm going to cancel it because I already installed it so that's where you have to download the Docker from and then let me install the Docker real quick and make sure this is this checkbox is uh on use WSL 2 instead of HyperV this should be turned on and then uh hit okay now it takes a few seconds and uh get installed we will be using Docker to run most of the tools in containers inside the Docker instead of running them on our host machine because it is more reliable and efficient it's like u a virtual machine VM so it's like a virtual machine uh that and it will run the tools in and virtual environment inside the containers now the Docker is installed let's close this real quick now let's install VS Studio download for Windows here as well i'm going to cancel that because I already have the file with me let me just Go to downloads the path and make sure you check this box add to path and then hit next installed after that finish so let's install a few extensions a couple of them that we need on the left pane click on extensions type in Python we need Python by Microsoft i already installed it on my PC so it shows uninstall and we also need Docker again this is also installed on my PC but you need to install it as well so we need that in our project that's it with the VS Code now let's open command prompt by the way you you have to have Python installed on your PC and then you also need to install you don't need to install this is just for the convenience it's totally up to you to install a virtual environment but I I install it to make it convenient for me so all the tools that I use in this project at the end of the project if I want to uninstall all the tools I don't need to uninstall one after another I can just take the whole environment down so the tools go out with the environment well uh it says requirement already satisfied which means I already installed it but if it's the first time for you to install a virtual environment on your PC it takes a few seconds to install so we have that and then now we need to initialize the virtual environment for that let me go to file manager C drive users username create a new folder and name it something like real time stocks and let's say MDS modern data stack that's good now let's initialize the environment inside that folder to do that we need to go to that path wait cd copy that folder name CD and here we initialize it using Python minus M V space V env now this initializes the virtual environment in that specified folder which means you will find something like this inside the folder that we created which means we initialize the environment here now we're good with this let's head back to we don't work with this uh right now we'll keep it for later but now let's open the folder that we created in here let's create another folder named infra referring to the infrastructure of the whole docker inside the infra we need to create a docker compose and it should be yml file i'll show you how to create it and this folder all the files and like I said all the files and uh codes that I use here will be available in the GitHub and the link is in the description so we have this now let's save as inside our project folder and inside infra here let's name it docker compose dot pi ml and uh file type to all files and UTF8 as encoding save it now that is saved head back to command prompt head to cd infra we're heading to the folder inside our project folder inside the infra where we need to initialize the docker containers here to initialize Docker we need to run docker compose up state well the error here is I ran that command without opening Docker we need to open Docker desktop first it should be running in the background like so we need to sign in first let me just sign in real quick let me pause the recording i'll just skip it right here so I haven't signed in i just skipped it so it should be running in the background now let's run the same command basically the compose includes zookeeper which we need to uh run Kafka and uh we are initializing the Kafka container and also CF drop which is the UI that we use to spectate Kafka and we use minio we use the minio container for the storage and Then airflow scheduuler to which is the like like it says it's a scheduleuler for the airflow and uh we need postgress and also airflow airflow web server which is the UI and all of these have their uh dedicated ports which are assigned for the cafka is 9092 and also we add another port 2992 2 and for calf drop it's 9,000 for minio it's 9,0001 9,0002 all these are basically the default ports that are assigned for them we can we can uh use a different ports too but this is the best practice to use these ports for that specific tool so yeah we creating se uh separate containers for every single tool here we me uh mentioned Zookeeper Kafka Calfrop Min.io Airflow Web Server Airflow Scheduler Postgress and now the tools are downloading let me just pause it and now it says all the tools all the containers for the tools are created and when you see that and you open the docker desktop here extend the infra let me make it full screen here you can see all the containers created now after that we need to initialize the airflow you see right here it they are not running continuously they stop frequently so to fix that we need to initialize that first for that let's run docker compose so we initialize that and then also Docker compose after a couple of seconds they should be up and running uh without any breaks okay now after you see all of them running without any breaks now we need to By the way I forgot to tell you something in our compost file we included in min.io we included the credentials here itself these are the credentials you can edit this credentials as you like so you can uh replace this user like uh let's say J and you can replace a password with whatever password you like we did that for the minio inside the compost file itself for but for the airflow we need to set the password after the airflow is uh initialized and running stably which is now so this is the right time to initialize I mean define the credentials for the airflow because it is stable now so to define the credentials The username I'm going to give it J first name also let it be Okay and last name i say K ro admin ro privileges and uh email my email and password and give a random password password the same password that I used for min.io password 1 2 3 you can put your desired username and your desired demographic information here then hit enter user J created with the role admin so that is done let's see if all the containers are working now actually so to access Kafka like I said calf drop is the user interface for Kafka so we have to click on the CF drop port is the user interface for Kafka this one and uh next for the storage minio we have to click on this port min.io And here username and password like I said the in the compos file it says admin and pass password is password 1 2 3 we need to put that in admin password 1 2 3 acknowledge here we can create buckets just like as in AWS S3 and now Airflow user interface is web server need to open that here we just created the username and password which is J and password is password two this is the homepage for airflow and this is for the min.io and this is for Kafka which is cafrop okay that is done so most of the tools that we need are initialized now and uh for the first step in our architecture which is data source we need to get the data source first for that you need to go to FinHub if you haven't logged in uh to FinHub you need you need to register first with uh unique email and uh and then they will provide you with a free API key on the homepage free API key which allows you to do 60 calls per minute this is the API key and you need to get that API key to get the uh stock data out of this uh fin hub API so we need this API key here okay so that is done and uh let's set back to docker no let's set back to cfdrop we need to stream our data into Kafka right so to do that we need to have a topic created in Kafka so we need to create a topic and let's name it stock quotes sim stock codes and partitions three to uh manage the loads and replication factor just like I said in our last project replication is uh about repeating uh your data storage locations so it it is basically about security of the data and uh to reduce the risk of losing your data you need to increase the replication factor in our case it's just a project so we just leave it one as a replication factor here and the partition has three and create that successfully created go back now the stock course is created here great well we have that now we need to let's open VS uh code we need to create the producer code first right we need the producer code and then we need to we need the consumer code to pull the data out firstly we need to create the producer code for that in the VS code head to search and uh now click control + shift + p like it says right here show run commands controll shift + p you need to do you need to click that control shift + p and you land on this one you need to search for python select interpreter right here need to select that and uh it shows all the environments and we need to select the virtual environment that we created uh right now we don't see that here because we haven't uh imported that yet let's just close this first on the top left hit on file click on open folder choose the project folder that that we created in users username real time stock inds now we'll find this I trust the authors we'll find the folders that are in our project folder now we need to head back i said control shiftp Python select interpreter now we can see the virtual environment that we created which is this and it also says recommended now we need to click on this the virtual environment that we created select that and uh now we need to write the producer code first and then run it in the VS code to get the data out of API and uh stream it on Kafka well before creating the producer code we need to install the tools that we need let's install them for that I created a requirements.txt file which basically says all lists all the tools that we need kafka Python for Kafka and Bordo 3 for uh S3 which is our uh storage in our case bin IO so this is border 3 is used to connect to the S3 bucket that we create uh in the storage account and then snowflake connector python is to connect to snowflake and dbt core uh for the db to run dbt and dbt snowflake to run dbt on snowflake and then we need requests okay so we need to install all the these requirements so I listed all of them in the txt file and this requirement.txt iss also will be available in the GitHub now to run that now we don't you can run that on command prompt as well or we can just go to here now let's just run that on command prompt command prompt right now we are in infra we need to go back a little bit so to go back we do cd dot dot and then virtual environment scripts we need to run this file so to run that venv slash scripts and then activate activate now the virtual environment is activated here we need to install no wait we need to first copy the requirements.txt txt file into our project folder so let's paste it here because we activated the environment uh we need to head back to the project folder the root folder now because that is where we pasted the requirements.txt txt so to head back we run this command twice now we're in the root folder now we have to write the command like pip install minus r we use that flag to write read our text file requirements dot txt this installs all the tools that were listed in the text file let me pause it and play it after the installation is done now the installation is done so now our virtual environment is ready to run our producer code so we need to write the code first let's open Python i'll just create I'll just uh do the imports in this text file in the notepad and I'll do the rest of the process of extracting the data in the VS code import requests from Kafa but half producer and now save this file inside the our project repository just here inside our infra let's create a new folder new folder name it producer yeah and save it here name it as producer 2 producer ey to specify it as a Python file and also change the type as all files inside the producer file and encoding UDF8 save it now inside our infra we should see the producer right here right here now here we run here we write the code firstly we need the API key we need to connect to the API which is our data source and we need we need to retrieve the data from there and we need to connect to Kafka and then stream the data on Kafka yeah that's it and by the way the data should be streamed in bytes we have to convert the JSON file into bytes before streaming into CFKA for now let me Let me just define the API key equals and uh copy the API key paste it in the quotes with URL for the base URL i think I uh copied it somewhere goes something like let me uh you just need to copy the base of base URL of this finub website paste it then it goes like api slash v1 slash code symbols for the symbols we'll be doing five symbols for this project which means five uh stock prices of five companies and for the companies let's choose the top companies that are on the stock list which are which are a part of NASDAQ nsft Google AMCN Amazon By the way this base URL is uh it can be f found in the API documentation in the finub should be somewhere around uh I think it's the API documentation yeah somewhere around here i downloaded it so that's where I found it well you can use that use the same URL too so I'm not going to find it again now we have five companies chosen have the link let's define the producer producer equals Kafka producer bootstrap server and for the port need the ports that we specified in uh the yml since this is outside the docker since we are running this VS code outside the docker we need to use this port to connect so let's copy that if you're running running this code inside the docker we can just use localhost and uh the primary port but now here we're running it outside the docker so to access it we need to use host.docer because this is the host PC this code is running on the host PC so we're going to use that as our bootstrap server check let's double that like I said serializer just type in real quick lambda json dot terms of Here the value serializer like like I said Kafka streams the data in bytes not the JSON files directly so here we're specifying JSON.dumps to convert a Python dictionary into JSON file we use this method to uh convert a Python dictionary into JSON file and then we encode it into bytes using encode UTF8 to make it available to stream on Kafka so this is the value serializer for that and this is the bootstrap server where we connect the port actually so it's like an address the a port is like an address for the tool to uh access let's create a function use F strings and URL open should be our API key set so this is the whole URL so at the beginning we just define the base URL that we need to connect and this is the whole URL which includes the symbol that we need to retrieve and also the token which is the C API key so this is the whole URL uh we need to use to get the data the the stock data of that desired symbol from the API and now for the exception handling I'm going to use try and accept method here for the try response create a response variable i spelled it wrong yeah response request dot get of URL response dot raise for status data equals response dot JSON of now uh should be JSON and now we add two more columns into the data we're getting as a meta data metadata which means a data about the data we're getting so in that we will include the symbol and also the fetch time when we fetched it because uh the data we're getting from finub doesn't include the symbol it just includes the close time all the all the biographic data except for the symbol inside it so we have we create a separate column to record the symbol as well so we have to include data as symbol data symbol symbol then data fixed at int remember we imported time we imported time because we uh you can use that module to get the present time we use time dot time off which is the present time and this retrieves the present time not not in eastern time or IST it's a UTC time so the universal uh time so it this method retrieves the universal time the universal present time in the UTC format did I say that right I think I did so we return data and now if we haven't got any data we raise an exception except exception as e and print so it the machine lets us know it doesn't it didn't have it didn't get any data from that API so for that let's create an f strings to display that error error fetching a flower bracket to include a variable which is our symbol and two and flower bracket of E which is the exception that's it and return none because we haven't got any data we return none from this function that's it now we connected to what we connected to the API we have the producer ready which can serialize which can uh convert a dictionary into JSON and encode it into bytes we have that and now let's create a while loop to iterate infinite times so to create an infinite loop we have to use while true method so this iterates the process this iterates what else whatever we say in the while loop infinitely unless something breaks it it should be a human or it can be a command inside it so we we're not going to use any command inside it to break itself we're going to use we're going to break the producer by ourself if we want to break the stream we can just close the command uh I mean the terminal the whole terminal so it terminates the code in the loop we use for symbol in symbols which we defined the list that we defined at the beginning with our five companies that we chosen and in that for loop code fetch quote of symbol which is our function that we created so we're passing each and every single once for the loop into the function to retrieve the record from the API right so we got the record and we assigned the record to this variable and now we need to create an if condition if quote we need to print what data we are injecting we need to know that so we're going to print that using fst strings i say print using a flower pack to include a variable say quote and that's it now we need to connect the producer to stream the data producer and we are sending the data sending the record that we have which is stock codes which is the topic that we created in uh cough drop you remember this one need to use this just in case I don't do any spelling mistakes now we specify the value that we need to push which is our code the variable that we uh got here that's it and uh we delay the loop for 5 seconds and why 5 seconds because uh FinHub for the free tier the free key that we get it only allows you to call 60 uh to have 60 calls per minute which means we can call we can use one call for every second so we can do that and because we are retrieving five companies at the same time we cannot do that for every second we have to do that for every 5 seconds because we are retrieving five records at the same time so we have to to be to make sure we are in the free trial limit we need to retrieve one one record for all the five symbols once in every 5 seconds i think I got it right uh it might sound confusing let me show you time sleep five you know what instead of five let's say six just in case if uh that doesn't that exceeds the given privileges for the free to your account so I hope you got it we are using a 5second delay or 6cond just in case just to be secure so we are using that delay because we only got the privilege to call 60 records per minute which is one record for every second and we are calling five symbols which are which means five records at the same time so we have to delay that for 5 seconds to be inside the boundary that the API uh key given the fin given to us so that's what we did here and let's save it now why is it red so because here if you check for the syntax errors I asked Chad GBT and we I did a few mistakes and firstly I did a typo here this one right here should be capital K for the Kafka producer and also I did a mistake at should be capital Kafka producer wait got it closer and also here I defined API key where ey is in small i mean where uh here I used API key where all of them are capitals and here I use lower case so I have to replace these with uppercase as well so that is fixed and uh one more thing is I forgot to include a comma here there should be a comma there and that's it let's run this code now and I forgot to mention right now uh my account is under penalty let me uh let me close this real quick is under penalty so they won't offer me the 60 records per second 60 records per minute because they uh panelized my uh host address the host IP address so to use producer in my PC now I have to use a VPN to hide my uh IP address you don't need to use that just because my host IP address is banned uh I would say panalized by them I have to use a VPN every time that I access the API key so I'm going to use a VPN for that now it is connected now let's run There you go so it is producing one record for every 5 seconds so our producer code is ready all we need to do is we have to check if the records is actually pushed into Kafka to check that and go to cough drop which is the user interface and click on view messages inside the topic now it shows just one message because it takes time right takes time to upload it to Kafka let's refresh that click on all messages there you go now we can see all the messages let's go back a little bit stock codes hit back to that again last two offsets view messages there you go so all the messages like I said it doesn't include the symbol we included manually right here at the end we include a symbol and also the fetched time this is the time that we uh got the well all the symbols I asked at GPT i mean I forgot so here it is well C refers to current price of that particular stock which here is Tesla's particular stock and right now it is at 346.4 four and this D is a change absolute difference from previous close the previous day's close and today's open uh no the difference between the previous days close and uh the price currently which is 346 and DP is a percent change from previous close it's the same thing like this but it is in percentage here and then H is the high price of the entire day and L is the low price of the entire day and O is the open price of the day and PC is the previous day's close price and the time stamp T is a time stamp of that particular uh stock symbol that we uh created and fetched it that we created the time stamp of when the data is fetched That's it that's with that's it with the data now we need to we have the data streaming which means we are we are done with this step as well we have the data source we have the data streaming on Kafka now our next step is storing the data in our S3 bucket which is minio console now for that we have to create a bucket let's create a bucket named let's say bronze transactions invalid bucket name oh I cannot use underscore I guess let me see transactions oh I cannot use underscore but I can use hyphen okay bronze transactions let's create that bucket now the data in the stream will be pulled from the stream and uh saved in this storage well that is that you might wonder why we particularly used FinHub because there are a few other APIs that can be even more efficient but you know there is Yahoo Finances well it is more uh efficient it it doesn't have any limits we can do any calls any uh any number of calls per minute but it is not official the data might not be real so Finnhub is actually a real API so this reflects the real data that is out there in stock market so I used FinHub to for the realistic uh for the to to make the data realistic and now we have the data streaming on Kafka we need consumer code to create consumer code in the same way that we created producer code we'll create a n We're going to create a notebook and we import JSON for the to deal with the JSON file and we import BTO3 like I said BTO3 is to deal with the S3 bucket storage space which is our minio console here so we need border three to uh deal with the S3 bucket storage space and we need import time for time functions and then uh from Kafka import we need to import Kafka consumer this time because it's a consumer Oh we need to the shit consumer let's just save this right now and uh let's create another folder for that new folder name it consumer let's save it inside that folder name it consumer as well consumer py and the file type all files encoding UTF save now we can see that in VS code the data is still streaming let's create the VS code under the consumer here it is let's pull that and place right next to the producer let's run that and now what would be the steps in consumer am I forgot i think I forgot to put the headings for the code let me just do that real quick say import without requirements avoid confusion define variables for API api initialize initial producer and this would be retrieving retrieve data looping and streaming looking and pushing to stream that's great same for the consumer import requirements s3 equals Go to three dot client and uh you have to define the port http P the code that we assigned in minio local host and we are using 9002 to connect to mean I support the port now this is the port that we're using to connect to our uh storage space and like I said the min IO storage is executes identical to Amazon AWS S3 so we use S3 here as the client and AWS access key which is our user ID for min.io which which we defined as admin and AWS secret key is password 1 2 3 which we assigned while creating the container right here the user and password now we have the connection let me just put a heading for that now that is done okay let's specify the bucket name which is the bucket name that we just created bronze transactions let's just copy that now now we have uh we assigned the connections for min.io now we have to define the the consumer let's define the consumer with the variable consumer consumer equals Kafka consumer and our consumer we have to consume the data we have to get the data from stock codes which is our topic that we created where our data is what am I doing bootstrap servers the host name for the host name let's copy the host name that we assigned i made bronze consumer yep that's it now we define the consumer for the Kafka consumer we are assigning stock codes which is a topic that we created in Kafka which is this one stock codes and then we define the port that we need to connect which is host docker internal 29092 which is in the compos uh yml file and this one right here we use this one uh to save the checkpoints you know like also the group ID is where the offsets are saved you know when uh we run the consumer code and it saves the data that is on the stream and let's say if that consumer code is interrupted for some reason and it has to run back so when it stops and uh it came back operational again it won't load the files I mean it won't uh get the files that are on the stream which it already processed it only takes the new files so it needs to save the offsets which are basically checkpoints where uh the process stopped so whenever you run the code again it starts from the point where it stopped so that's why we use enable auto commit equals true and we save those offsets in the group ID which is bronze consumer and this one here you remember we used value serializer in producer right here right here we used value serializer where we convert we converted the Python dictionary into a JSON file using JSON dumps and encoded into bytes using encode UTF8 now we need to decode it because the stream is in bytes in Kafka now uh to get that out and save it in minio we don't we cannot save bytes right we have to save JSON files so we have to convert them back again which is d serialize that to des serialize that we do json.loads loads of off which is basically converts which is the other way around whereas JSON dumps off converts dictionary into dictionary into JSON file JSON JSON loads of converts JSON into Python dictionary it's the other way around and uh we use V decode to decode the bytes where we use encode in the producer we use decode here to decode the bytes into the actual JSON file that's what happening here and now we define the consumer we have the minio connection as uh defined already now what else we have to do for the consumer now we have to pull the data and uh save it in the storage space before going to next step let's use print here so we will know if anything wrong anything goes wrong in the top code anything goes wrong uh in the code we written so far we'll know there's something wrong in the top part of the code if this statement this syntax this uh string is not printed we will know the error is on the top part of the code that's why we do a print here streaming say streaming and saving to min IO say three dots like now we have to loop we have to get the data out of the stream and then store it let's say main function we use a for loop message in consumer record equals message dot value symbol equals record.get Get off simple wait ts record dot get of say fetch that in int of time dot time of like I said we use that method did I use have we used the time dot time of method to get the current time that is in universal format UTC we put another bracket correct and we create a key using frings which is dynamic slash this is basically formatting the file name the JSON file name so our JSON file name will be a combination of symbol with the fetched uh time and now the uploading part uploading part to the S3 bucket object that will be for this bucket equals bucket name bucket name key equals the key we already defined and for the body JSON dot dumps off we're converting the dictionary again back to JSON file before saving it into the bucket terms of the record content type content type equals application / JSON and now yeah that's it so this is the main function we're getting message we're getting uh we are looping this function for every uh message in the consumer here so every message in the consumer we loop uh through this uh loop and uh we assign the variable record with the message do value basically a message in consumer gets one single record one single record from the consumer but still that single record also consists of metadata that is assigned to the data uh in the CFKA so CFKA basically uh it assigns metadata when you stream data so it automat it automatically assigns a few metadata to it like uh the time stamp when the record is generated and time stop time stamp of when it is retrieved so the stuff like that so we don't need that stuff we only need the message that we that is inside that so we to select that we use message value that's the value that we actually need the data that is actually streaming okay so we get that value and assign it to record and symbol equals record dot get of symbol and we get the symbol out of that record and uh the timestamp the current time stamp and then we create the key we are just formatting the file name for every single record so it so it will be unique it will be a unique file name so for that we have to do we we do symbol with the the fetched time combination of both and then we push them into the S3 bucket put object bucket name bucket bucket name specified already defined here and the key the file name which is already defined here and body JSON.dumps dumps off we did JSON dot load soft here to convert to dictionary and then here we converted it back to JSON and then content type application JSON that's it so to make the code let us know it executed fine it has to print something for us frings Use a dynamic string here record for include the variable symbol equals S3 slash the bucket name bucket name slash key no key that's it let me check if we have any errors in this code now here are the errors that I found i wrote con customer instead of consumer consumer streaming and saving email and one more thing is I forgot a comma here forgot a comma and that's it now let's run this code and see if it works because our producer code is already running in the terminal let's click on the arrow right here and click run Python file in a dedicated terminal this will open a separate terminal to run the code while the producer is running on the other terminal consumer is running in this terminal oh I just found out some another error here it just takes the latest files and it avoids the old ones which means let me just stop this real quick which means did something wrong with the the consumer right here supposed to have offset reset equals earliest so this one it makes the code consider the old files as well the old files that are saved as well so without this it just takes the new files that are coming in all these all these uh files in the stream that are old will not be considered so we have to use earliest here auto offset so offset checks the earliest files as well now let's run this again in a dedicated terminal now it should save all the old files first and then come to the new one let's see how many files are actually in Kafka 139 and minio we if you refresh that we'll see all the folders created but it only shows one file for some reason there you go tesla has to Google one Amazon one one I think it's because my VPN you know VPN makes my internet slow I think that's the reason here I just found code this happens it gets stuck here because I already ran this code before and this is the second time I'm running this so the first time when I ran the code it already saved the offset in bronze consumer so the second time it assumes it already has the data with it so I have to change I have to delete the group uh that is existing in the Kafka or I can create a new one so let's just create a new one say bronze consumer one here now this should fix the error now delete this and run this again now all the old data and the new data will be saved and I by the way I just stopped the producer code i I had enough data so I stopped it and also I don't want to run this VPN in the background i just so the producer is stopped for now we have like not sure how much how many records we have we have 139 records that's fine now our consumer pulled the data from stream and stored it in the bucket so let's check that out now refresh it there it is all the data that is saved in JSON format for all the five companies that we wanted now we have the data in the storage our next step would be transferring this data in the architecture we're done with the data source we're done with the data streaming we're done with the data storage now using the consumer now we have to pull the data out of the data storage account and store it in snowflake in uh the bronze tier table and then we have to go through the data processing step let's do that and let me just I think I already got all the files that I need so I just let's just close this to save the computational power okay for this part we have to we need to open the snowflake account snowflake home the data let me delete this data real quick because I already created that before let me drop the whole database i just dropped okay in Snowflake head to home by the way don't mind me uh getting more errors a lot of errors by the way I just realized I've got a lot of errors while working with the codes i know there are a lot but if you while you are doing this project if you don't want to get any of those errors just download the codes from GitHub and run them because they're all tested and there is no way you get any error if you run that code so just download those codes and take a look at them they are they're just perfect well for the next step we need to like I said we need to uh pull the data out of the S3 bucket and store them in the snowflake to do that let's open the snowflake first the homepage click on create SQL worksheet now we have to create a database and a table for the bronze create database stocks MDS all right and uh create schema stocks MDS dot common this will create uh a database and also a schema inside the database let's run that first and see if it works syntax error oh I use the colon instead of semicolon another error now run that schema common successfully created now when we refresh this we will see a new database that we created named our stocks MDS and named MDS because we are using modern data stack so I'm naming everything with MDS as a suffix and a common a schema named common is also created now we need to create a table inside this create table bronze make sure it is inside the schema that we just created this one right here make sure we selected our schema which is stockmds.com make sure this is here and now we can run this directly stocks quotes v as variant i think I typed it wrong right here this one this should work okay and now we created a table with one column as V which is variant now when we refresh this we can see the table that we created under the wait under the the schema common under the tables bronze stock codes raw now we'll the data we pull from the bucket and for the next part to pull the data out of the S3 bucket and upload it to the snowflake we're going to assign that job to airflow using a Python code and I already have the code written let me show you that right here this is the code that I used let me uh well I can't write the whole code on the camera because it takes a while because it's so long i'm going to explain what we're actually doing here so we are here we're importing the requirements and here configuration for the minio connection where it includes the port that we connect and uh username password bucket uh name a local directory where uh the data gets stored temporarily before it pushed into snowflake this is the folder name for that and this is the configuration for snowflake which includes the username and my actual password which is my temporary password i'm going I'm going to uh change that password again so this should be your uh snowflake password and this should be your username and uh snowflake account and this one you can get this one from Snowflake on the left pane click on the username at the bottom and then click on session details right here snowflake context you have to copy this part and paste it here just this part and then uh Snowflake warehouse compute WH which is the warehouse and uh to find your warehouse name you have to go to the admin click on admin and uh click on warehouses you'll see all the active warehouses and in my case it's computouble it's mostly computoubleh so this is the warehouse that we're using And uh yeah we copy paste it here and then uh snowflake database the database uh that we just created which is stocks MDS that's the exact name stocks MDS and under that the schema name common snowflake schema common that's the configuration for snowflake and uh there's the definition downloading uh for downloading For from minio this the function to download data from minio we use os do make directories local directories existing okay endpoint endpoint URL midpoint uh we just assigned the variables before for the minio connection here we are actually establishing the connection using this and calling the variables that we are already defined minio endpoint access key secret key that we already defined here we're calling them here to establish the connection with the bucket S3 bucket and then objects and here we're assigning the bucket the contents local files we are creating an empty list for the local files and uh a loop key equals local path uh post path local directory path first name well we're just basically downloading the files and uh assigning to local files and then sending it to what uh yeah that's it we're returning the uh the variable local files which has all the data that is in the S3 bucket and then here this is the function to load the data to snowflake and uh yeah this is the configuration connection configuration right here it establish the connection using the variables that we already defined at the top of the code and uh in this loop it accesses the staging table so whenever we create a table in snowflake a staging table is also automatically created it don't it won't be visible to us but it will be created automatically so and the name of it we can uh we can access that staging table using this uh keyword at the rate percentage and the table name which is our actual table name and then cursor.execute execute allows us to run an SQL code inside Python and it says copy into bronze stock codes raw which is our table from the staging table that we already loaded the data into and the file format is specified as JSON that's what we did and we printed that for the confirmation copy into uh executed and we close the cursor and we close the connection And this is all for the airflow bas it basically says owner is airflow and depends on past false and then start date uh this is the date that we need to specify when we are starting the the airflow the orchestrated uh DAG DAG file so today's date is 9 9 so I'm going to change it from 5 to 9 so that should be the today's date retries equals one so if the code fails if the DAG code fails how many retries should it perform so we are specifying one just one retry and then it says fail retry delay will be 5 minutes and minio to snowflake tag here we are uh calling the function minio to snowflake no no no we are calling the file name min.io to snowflake and schedule interval specifies the time interval now these not every 10 minutes this for 1 minute so this is for every 1 minute it executes for every 1 minute if you do this or else we can say this so it executes for every 10 minutes now this means it executes for every 1 minute we're scheduling it uh to run for every 1 minute so catch up false as tag task one perform now the task one includes performing the download minio function which is the function that we defined here so that is the task one and task two will be load snowflake which is the second function that we defined which is this load to snowflake and here we are defining the flow of the process so first you need to uh execute the task one and then you go to task two that we're so that's what we're saying to do now let's just save this file save as and we have to save that into C users infra and when we initialized our airflow it creates a few files including DAGs logs and plugins we have to in save this file in DAX because this is a DAG file for airflow switch it to all files say py and save now it is saved when you save the file in DAG uh folder it automatically be visible in airflow but you have to wait a few seconds until it refreshes there it is so it takes a few seconds to refresh now our py file which is our DAG is visible in Airflow switch it to all we need to activate it now right when I act right when we activate it it start executing so now it is running we can see it here and already one execution is already successful and uh yeah here this displays all the DAGs and if you want to see the log files and audit them we can go to here browse and audit logs this shows all the activities that are done in the airflow now here it says running it uh it also shows the success uh the successful run that is already done by the DAG and the time when it is successful which is 519 uh now the time is 519 uh the successful run was at 1718 which is 518 and we can also check the logs when we click on the DAG here audit log right here now it shows running there is a successful uh run already refresh this so there are two successful runs which means we should be able to see the data inside our bronze table that we created we head to data here databases inside our database inside our schema and the table that we created this one there you go shows four 12 records already pasted data preview there's all the data so the all all the data is now inside snowflake which means we're done with this step we got the raw data into the bronze table now we need to clean the data and store it into uh silver table and then we need to transform the data which is business ready to be stored in gold all this data processing should be done with dbt to initialize dbt let's head to uh C drive users our username and our repository the project repository create a new folder and say DBT stocks now we are going to initialize the DBT inside this folder to do that let's open command prompt uh activate the virtual environment first to do that go there go there copy that going to activate the virtual environment activate it now let's head back to DBT stocks right now the command prompt is in uh our project folder now we have to go inside the dbt stocks and here to initialize dbt I have to run dbt in it dbt initialize and uh folder name as dbt stocks enter a number which database would you like to use it says snowflake is number one in our case it is snowflake so number one and account for the account we need to head back to snowflake and here click on the session details we have to copy the same thing that we used to connect uh the snowflake snowflake before just the this part and paste it here and then user your username and select one to use password and type in the password that's right and for the role say account admin for the warehouse the warehouse that we already have should be on the left pane click on the warehouses just like before copy the warehouse name paste it here and the database that we created which is stocks MDS and the schema is common threads say four and now we're all set in light DBT which means if you open the DBT folder that we created that we created we'll see these two folders and inside the whole we'll see all these files which means it is initialized successfully now all the models all the transformations that we do should be stored in this models right Here here we head to models now first we need to structure all right let me rename that again i don't want the capitals small i'm going to create one folder for bronze another for silver and another for gold why did I created bronze when I already have a bronze table there because the bronze table we have here we stored the data as a single line we haven't structured it we haven't passed it in the right way so all the data that is stored in there the records that are already only saved in one column which is V that we uh defined when we created that which is V of data type variant which is not structurally structurally correct we have to pass it accordingly and uh assign data types to that so we created a view for br we have to create a view for the bronze that's why we create a folder for bronze silver and gold now first let's create an SQL code for bronze before creating the bronze transformation we have to define the source first so like that sources name we name the source as raw database our database name which is stocks MDS by the way this is uh this is not case sensitive our database we can uh we can say u we can write that in lower case or upper case doesn't matter so that's our database and our schema name not this one this shouldn't be in capitals just The names can be in capitals schema which is common and then table that is our bronze table so we are referencing the bronze table that that is already in there in the snowflake and the schema and the database and the source name is defined as raw now we need to save this inside DBD stocks stability stocks models bronze and say sources dot yml Okay we specified the source now like I said now we need to pass the data if you're wondering let me show you what I mean if I do select Asterric which means all from the table bron stock codes raw now limit equals 10 if I run this it should be limit 10 that's it not equals now if I run this see all the data is just stored under one column which is V which we defined as variant so it doesn't have a proper structure at all now we need to pass it we need to structure this data in order to do that I'm going to I'm going to write a query here but I'm I'm not going to execute the query in the snowflake i will copy paste it in the notebook and execute it in the uh while we do the dbt run but let me just write it here because it is convenient select V is to C so we are selecting from the column V selecting C defining it as float as current price so we are aliasing it with current price and then V E S change amount then VP it's also a float S change percent V H float Yes they high Okay open previous day close time stamp as market time stamp is symbol which is string as market symbol just a symbol we fetch that extract time stamp as fetched And uh we define the source from source of the source name that we created which is raw and uh the table bronze stock quotes raw that is it let's now cut it paste it in a separate notebook notepad click on save as head to bronze d in the bronze now name it bronze STG stock switch the type to all files have the YML file UTF8 save now we have the bronze now we do the silver transformation which is cleaning unlike the other projects we're not going to implement SED2 or slowly changing dimensions or schema evolution in this project because we don't have to schema I mean SED2 slowly changing dimensions is doesn't make any sense with the stock data and uh we don't need to implement a star schema for this because all the data that is u in the table is all related to one particular entity and it should be compact uh it should be combined it should be uh enclosed clustered in just one single table we don't need a few other dimension tables to uh refer that's why we are now going to use star schema and also sd2 with that being said for the cleaning step because we are getting the data from an official site we don't need to do a lot of cleaning But if we see here all the decimal values are have a lot of decimal points we can just clean them or we can just change them to only have two decimal points we can do that in cleaning so yeah let's do that for the silver cleaning say select symbol current price and then we have Current price very simple with current price then what we can do change amount yeah let's do that let's go with a day high let's round it up with a high comma two decimal points as day high and round off day code also should be two decimal points as day low round of day Open comma 2 as day open round of previous close round it up to two as well and say previous close change amount change amount can be same but round Change person as change percent and last uh lastly we have these two market time stamp fetched at from RF the bronze staging table that we just created the name is bronze staging stock codes let me just copy the name can reference it using that and uh say where current price is not null let me see if we have any errors in this well I found a few errors right here the total round should be capital i left some with the lower cases and also here this should be lowerase C and uh also here referencing this should have a space in between the brackets between the flower brackets and the parenthesis now that's perfect create a notepad a new notepad paste it now save it save as models this is the silver uh transformation and uh for the name you can say silver clean stock codes dots SQL by the way whatever the name that you specify here for the file name will be the name of your view that will be created in the snowflake and yeah so we have this save it now we have the silver table now we need the gold transformation which is which will be our business ready transformation let's head to that me close it real quick erase the whole query for the goal transformation like any other dashboard we will be creating a few KPIs right and because this is about stocks we will be creating KPIs for the current price of all the symbols that we chosen all the companies that we chosen so we're going to create a KPI for the current price for all the companies so for that we need a view like this select symbol current price change amount symbol order by fetched S R from close brackets space RF of Silver clean stocks and space here where R equals 1 well we are selecting the symbol the current price change amount change percent for the KPIs and uh a few more charts and now we are creating a view for that in case you don't know why we are creating the views inside SQL that is because we are in this project we're going to connect PowerBI with our SQL warehouse which is Snowflake using direct query method and not import and when we use direct query method it is always a best practice to create views inside the warehouse itself so the heavy lifting is done at the database level and PowerBI will only deal with the visualizations if we do the calculations in PowerBI PowerBI have to turn this calculation the DAX code into SQL and then run uh that SQL code in the warehouse and then get the output out of it because it is direct query it should be live the connection will be live and dynamic so it is always a best practice to create views inside the warehouse itself when you connect PowerBI using direct query method and that's what we are doing here and this is our first gold transformation query so let's save it as the back gold let's name it gold KPI.SQL and save and for the next uh two v uh two uh views I'm going to create they're big they're so big I mean not so big they're big and if I if I write that myself right now and I explain it it is going to take a while so I'm going to copy paste that views that I already created which one view is for the tree chart and the other one uh the other view is for the candle chart so I'm going to copy paste that code and I'm going to explain what we did in that query okay i mean I don't want to make this video lengthier it is I think it is already a lot bigger than what I expected so yeah I have to make it faster now take this off and uh minimize this minimize this stocks have that right here candlestick this one right here well uh the reference is already here it took reference from the silver table and then uh it creates it selects the symbol uh cast market time stamp as date which means it only casts the time stamp which is uh the a time stamp contains the date time with the seconds but here we're just taking the date so we're just casting the time stamp as date and we are assigning we are aliasing that with trade date and then we select a day low day high current price first value over we're partitioning the symbol by the date we're ordering that uh by the date so we get the latest value no no no wait wait wait wait no no no no we get the old value which is the first value and here in the last value we we're going to order that with the time but this time we're going to order that descending in the descending order I guess as date order mark time stamp cross between unbound proceeding and unbound following Well basically we are uh getting the opening candle and the closing candle for the chart and then candles here it says minimum of day low max of day high any value of candle open candle close and from that candles we're getting we're selecting all and partition by symbol order by candle time descending then we're selecting candle uh time candle time low candle high candle open candle close and a trend line to see the trend to monitor the trend in the candlestick chart which is a line drawn over the candles to see the trend and uh we are making sure the candle chart only shows 12 candles on the chart that's why we use this in this line and that's it uh this is the view that we created for the candlestick now let me save it inside models no gold gold candlestick sql all files save and also the tree chart close this yeah the same thing we're going to cast the market timestamp as date just the time we we're just going to extract the date out of it and earlier it as max day and referencing from the same silver table silver view that we created and latest prices select symbol average of current price as average price we're calculating the average price then join latest day ID market time stamp as date 1 day max day and here we are calculating the volatility volatility is nothing but a measure of how often how much the price is fluctuating for uh the given symbol or the given stock the the given company we're going to calculate it using standard deviation we're going to calculate the volat volatility with standard deviation and we are also going to calculate relative volatility which is nothing but making the volatility stay between the scale of U scale from 0 to one because the actual volatility would go any um anywhere from 0 to yeah the uh the actual value of the volatility would go anywhere from 0 to infinity so we calculate the relative volatility to make the volatility stay in the scale from 0 to 1 that is that and now that's it join the alltime volatility and yeah this is the tree this is the view that we use for the tree chart that we're going to create and now let's save this save us the same spot users models called dossql at the end and files all files save now we have all the transformation uh views the queries for the views stored in the models right here so let's just run the dbt to run that we need to open oh it's already here open command prompt and here the actual DBT is initialized inside this one so we have to go more deeper which means we have to look at CD DBT stocks here we have to run the dbt here to run dbt we type in dbt run and hit enter four thread seven models and all the seven models executed successfully which means we should be able to see all the views listed in the snowflake right here we need to refresh this under the data and uh extend the dropdown that says views and you can see all the data here the bronze data the gold data the gold candlestick silver all of them is here now we need to connect PowerBI to Snowflake to get this data and we're going to use like we said we're going to use direct query method here by the way we are getting the stock data of NASDAQ which means we have to make sure you run that uh the stream on the right time because the NASDAQ stock market is only active from 9:30 a.m to 4:00 p.m eastern time if you run the stream uh when the market is closed this what happens in my case uh the volatility will be zero and the relative volatility will also be zero for the most of them because the market is closed when I run the stream and if you take a look at the the data we have in silver you see the whole stream retrieved the same um current price it never fluctuated 237.88 for Apple the same repeats all the every time see here 237.88 and here 237.88 88 so that's what happens when you run the stream when the market is closed so make sure you run the stream when the market is active that is from 9:30 a.m to 400 p.m eastern time so now I have nothing to do i just need to wait a few more minutes until the market opens and uh these values updated that's what I can do right now uh so I'll wait and once you get the data the actual data where the stock market is active you can find something like this this is the real volatility and relative volatility these are the values to connect PowerBI let's go open our PowerBI first here click on get data from Click on database and look for Snowflake and hit connect and for the server name go back to Snowflake click on session details but this time we're going to copy the whole thing except for the https and paste it here and also copy the warehouse name paste it in the warehouse click okay and if you're connecting to Snowflake for the first time with PowerBI it will ask for the credentials type in the credentials connect now it is connected now we can see all the databases that are in there and under our database that we created under common let's import all the gold views that we created they're all there and select all of them and hit load and here we are going to use direct query like I said to make uh the dashboard real time uh to make the dashboard dynamic change according to the new data that is arriving into the warehouse so we're going to use direct query here click okay yep uh we got all the data and yeah let's check uh on these views if they're correct now we have all the views and uh yeah the data is also retrieved which means we're done with all the steps that are uh in our data pipeline all the way from data source to connecting to PowerBI so the data engineering part is all finished now if you're here for just the data engineering part it's pretty much it if you also want to create a visualization it might not be so visually appealing but I'm going to create a basic dashboard now so if you also want to look at the dashboard you can still stay or I'll see you in the next video okay firstly for the KPIs let's create cards for the KPIs let's make it quick i don't want to take too long for the visualization part so uh for this say current price we create a KPI for current price is now current price and uh we need to filter that because we're going let we going to create five KPIs for separate separately for each symbol so that we're going to filter just for the apple for the first one uh it's great let's just edit this a little bit category label turn it off and uh title turn it on title N Apple AP let's make it all capitals apple align it to the center color black doesn't matter now that looks good and the format I think I have to change the format too yeah that is it so for this where should I change the format to change the format we have to go to data format format custom format code 0.00 which is two decimals dollars a dollar sign at the end looks good isn't it that's great but the size is a little too big let's reduce that 20 should be fine that looks visually appealing now copy paste it five more times one two three four i mean four more times align them right next to one another for the second one edit the filter chose Amazon no choose Amazon that changes for Amazon you know what for the title I should have just gone for FX field value and the field would be symbol that automatically assigns apple the name apple to it so I don't need to do that for every other thing copy paste paste paste paste paste one two three four we don't need this remove that that there there and there and there the next one for Amazon and the next one for Google why is it changing why the ID is not changing i think I did something wrong here see the effect I selected the first one now uh that's unfortunate okay let's edit that and say amn CN Amazon this is Google And this will be Microsoft MSFT and this will be Tesla esla perfect now let's create a god chart for all the KPIs which measures the volatility and put right under this one and for that the volatility is measured in the gold the tree view select the volatile there and this the total volatile it says sum okay now we need to add the filter again simple apple that's good let's edit the array little bit data labels target label okay let's take out the title i'll just edit that title and see now what let's just name it volatility just turn the title off this should be fine press it right here like this and third align keep them aside for a while copy paste it and that is for Amazon then Google copy paste it google then Microsoft here that's just align them real quick that's good and uh for the candlestick chart click on these three dots get more visuals well PowerBI by default doesn't have a candle uh candlestick chart so we're going to import one candlestick i would choose this one get it import successful and now click on it and for that let's add axis should be the candle time and candle close to the close candle high to high candle low to low candle open to open come on there it is but here's the problem the data we have the stock market data it shows the candle high candle low and uh candle open candle close they only refresh once in a day and uh here I got two of them because I ran the code uh when the market were close which is the Monday uh data and the Tuesday data so I got two datas but if you are uh if you ran the stream when the market is open you you'd only get one candlestick here but I got two candlestick because I ran the data before even the market open so I got the the Monday's data and the Tuesday data as well uh in that case yeah in that case if you want to get more candles you need to uh run the stream for a few days at least a couple of days to get at least two candles or else you'll just end up having one candle now I have two candles here it shows the candle close candle high candle low yeah that is that and uh just edit the title real quick candlestick line it to the center well that is what we got and uh let's do the tree map right here with the tree chart symbol as category and uh relative volatility as now wait average price as values there you go and uh for the relative volatality let's just edit the color to show them for that uh click on colors effects and uh let's use rules symbol choose tree and uh choose the relative volatality summarization sum if value greater than or equal to No this shouldn't be no no no no sorry the format style should be gradient here and the field should be relative volatility and uh the lowest value let's change the color to red i mean light red right here and for the highest value choose the dark red now that should change there you go so the volatility uh relative volatility increases the thickness the intensity of the color increases and the size is depends on how that the current price of the stock which is MSFT which is a bigger uh stock price which has a bigger stock price that's correct and now let's give it as a little uh touch for the title too and say tree chart like I said I'm going not going to uh invest more time on this uh visualization part because it this project is basically is mainly focused on data engineering one so I'm not going to waste a lot of time on analytics part here but I'm going to do the basic chat just if you're so interested in it if you are uh interested in creating a good interactive good-looking interactive and uh and a chart with a more ch a more a dashboard with more charts you can take your time to do that okay we have that what can What else can we do we can do one more chart let's do a column chart clustered column chart using change percentage and symbol that should do and for this let's give it a touch a little bit of touch to look it to make it look good for that let's go to columns all FX here rule goal KPI change percentage greater than or equal to zero less than or equal to Say 100 i'll leave it 100 it should be green perfect and new rule greater than greater than 100 and less than zero then red what is this oh this should be number there you go now this shows the change percentage of the symbol if it goes up the change is positive it shows green and if it is negative it shows red now that is perfect and uh for x-axis hide the title for y-axis hide the title and the title can be change change percent align it to center and the data format can be percentage 50 50 what no no no no that is not the way change can be custom i'll craft it 0.00 and percent you know what i'm going to leave it actually 0.64 0.75 well it is what it is and now yeah that is that and uh let's add a slicer to filter the whole dashboard according to what we select in this slicer to do that we have to head to data model we need to connect all these views to each other drag the symbol from one view to the other and then cross filtration should be both side do the same with the other one and cross filtration should be both perfect head back here now from any of the table first create a button slicer choose symbol column from any of the view and now place it right here at the top let's test this if we click on Google everything filters just to Google if you click on Amazon switches to Amazon Apple switches to Apple here you can see the candlestick clearly so if you run the data for a few days you can see multiple candles in this candle uh stick chart you see that now that is looking good and uh if we edit the buttons for the title we don't need the title i guess now anybody would understand that the callout values can be aligned to the center that looks good yep that's it for the dashboard and now let's have a recap what of what we did we started from the data source and uh we streamed the data on CFKA and we pulled the data out of the stream and stored it in S3 bucket which is min io uh UI which we use and uh then we pulled the data out of the bucket and stored in the raw table in uh snowflake and then we went from raw table we cleaned it and stored it in silver and then we stored it in old and this whole transformation is done using DBT as a data processor and snowflake as a data warehouse and we used Apache Airflow to orchestrate the transformation from of the data from S3 bucket to Snowflake and then we connected the data to PowerBI now the whole pipeline is real time you don't need to manually uh run those uh the process this is all automatic and it doesn't need any human intervention which is why we call it realtime stock market data pipeline and now we are officially done with this project realtime stock market data pipeline which is based on modern data stack and I will be back with another exclusive project probably on the modern data stack again and by the way if you like the video don't forget to follow and with that being said until next video adios